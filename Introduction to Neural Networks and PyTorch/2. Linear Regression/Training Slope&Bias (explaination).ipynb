{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Simple Linear Regression with Gradient Descent in PyTorch\n",
        "\n",
        "This notebook will guide you through the implementation of a simple linear regression model using PyTorch.  \n",
        "We will use gradient descent for optimization and employ a custom visualization class to help you understand the model's learning process both in the data space and in the parameter (weight/bias) space.\n",
        "\n",
        "---\n",
        "\n",
        "## Table of Contents\n",
        "\n",
        "1. [Introduction](#introduction)\n",
        "2. [Importing Libraries](#importing-libraries)\n",
        "3. [Error Surface Visualization Class](#error-surface-visualization-class)\n",
        "4. [Creating a Random Dataset](#creating-a-random-dataset)\n",
        "5. [Defining the Model and Loss Function](#defining-the-model-and-loss-function)\n",
        "6. [Training the Linear Regression Model](#training-the-linear-regression-model)\n",
        "7. [Experiment: Higher Learning Rate](#experiment-higher-learning-rate)\n",
        "8. [Summary](#summary)\n",
        "\n",
        "---\n",
        "\n",
        "## Introduction\n",
        "\n",
        "Linear regression is one of the simplest and most fundamental algorithms in machine learning.  \n",
        "It models the relationship between a dependent variable $y$ and an independent variable $x$ by fitting a straight line ($y = wx + b$).  \n",
        "The goal is to find the optimal values for weight ($w$) and bias ($b$) that minimize the prediction error on the training data.\n",
        "\n",
        "In this notebook, we will:\n",
        "- Generate synthetic data that follows a linear relationship with some random noise.\n",
        "- Implement a simple linear regression model from scratch using PyTorch tensors.\n",
        "- Use gradient descent to iteratively update the model's parameters and minimize the mean squared error.\n",
        "- Visualize both the data and the learning process, including the error surface in $w$/$b$ parameter space.\n",
        "\n",
        "---\n",
        "\n",
        "## Importing Libraries\n",
        "\n",
        "First, we import all the libraries that are required for numerical operations, visualization, and PyTorch.\n",
        "\n",
        "```python\n",
        "# Utilities for Jupyter/IPython (not strictly necessary for the code to run)\n",
        "from IPython import get_ipython\n",
        "from IPython.display import display\n",
        "\n",
        "# Numpy is used for numerical operations and efficient array handling\n",
        "import numpy as np\n",
        "\n",
        "# Matplotlib is the go-to Python library for plotting and visualization\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# mpl_toolkits.mplot3d is required for 3D plotting (used for visualizing error surfaces)\n",
        "from mpl_toolkits import mplot3d\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Error Surface Visualization Class\n",
        "\n",
        "To better understand how gradient descent works, it's extremely helpful to visualize the \"error surface\"â€”that is, how the loss (mean squared error) changes as we vary the model parameters ($w$ and $b$).\n",
        "\n",
        "The `plot_error_surfaces` class below constructs a 3D surface (and 2D contour) showing the loss for different values of $w$ and $b$ on your data. It also keeps track of the historical parameter updates during training and can plot the path that gradient descent takes.\n",
        "\n",
        "**Key features:**\n",
        "- Computes and stores the loss (mean squared error) for a grid of $w$ and $b$ values.\n",
        "- Plots 3D surfaces and 2D contours of the loss.\n",
        "- Tracks the path of parameter updates during training.\n",
        "\n",
        "```python\n",
        "class plot_error_surfaces(object):    \n",
        "    def __init__(self, w_range, b_range, X, Y, n_samples = 30, go = True):\n",
        "        \"\"\"\n",
        "        Initializes the visualization class by creating a meshgrid of w and b values,\n",
        "        computing the mean squared error at each point, and plotting the initial surfaces.\n",
        "        \n",
        "        Args:\n",
        "            w_range (float): The range for the weight parameter w, from -w_range to +w_range.\n",
        "            b_range (float): The range for the bias parameter b, from -b_range to +b_range.\n",
        "            X (Tensor): The input features (PyTorch tensor).\n",
        "            Y (Tensor): The target values (PyTorch tensor).\n",
        "            n_samples (int): Number of grid samples along each axis for the surface.\n",
        "            go (bool): If True, plot the surfaces immediately.\n",
        "        \"\"\"\n",
        "        W = np.linspace(-w_range, w_range, n_samples)\n",
        "        B = np.linspace(-b_range, b_range, n_samples)\n",
        "        w, b = np.meshgrid(W, B)    \n",
        "        Z = np.zeros((n_samples, n_samples))\n",
        "        self.y = Y.numpy()\n",
        "        self.x = X.numpy()\n",
        "        # Compute the MSE at each (w, b) pair in the grid\n",
        "        for i in range(n_samples):\n",
        "            for j in range(n_samples):\n",
        "                Z[i, j] = np.mean((self.y - (w[i, j] * self.x + b[i, j])) ** 2)\n",
        "        self.Z = Z\n",
        "        self.w = w\n",
        "        self.b = b\n",
        "        self.W = []    # To store the sequence of w values during training\n",
        "        self.B = []    # To store the sequence of b values during training\n",
        "        self.LOSS = [] # To store the sequence of loss values during training\n",
        "        self.n = 0     # Counter for number of training steps\n",
        "        if go:\n",
        "            # 3D surface plot of the error surface\n",
        "            plt.figure(figsize=(7.5, 5))\n",
        "            ax = plt.axes(projection='3d')\n",
        "            ax.plot_surface(self.w, self.b, self.Z, rstride=1, cstride=1, cmap='viridis', edgecolor='none')\n",
        "            plt.title('Cost/Total Loss Surface')\n",
        "            plt.xlabel('w')\n",
        "            plt.ylabel('b')\n",
        "            plt.show()\n",
        "            # 2D contour plot of the error surface\n",
        "            plt.figure()\n",
        "            plt.title('Cost/Total Loss Surface Contour')\n",
        "            plt.xlabel('w')\n",
        "            plt.ylabel('b')\n",
        "            plt.contour(self.w, self.b, self.Z)\n",
        "            plt.show()\n",
        "    \n",
        "    def set_para_loss(self, W, B, loss):\n",
        "        \"\"\"\n",
        "        Record the current parameter values and loss.\n",
        "        Call this after each update step in training for visualization.\n",
        "        \n",
        "        Args:\n",
        "            W (float): Current value of weight parameter.\n",
        "            B (float): Current value of bias parameter.\n",
        "            loss (float): Current loss value.\n",
        "        \"\"\"\n",
        "        self.n += 1\n",
        "        self.W.append(W)\n",
        "        self.B.append(B)\n",
        "        self.LOSS.append(loss)\n",
        "    \n",
        "    def final_plot(self):\n",
        "        \"\"\"\n",
        "        After training, visualize the path of parameter updates on the error surface.\n",
        "        Shows both a 3D wireframe and a 2D contour plot.\n",
        "        \"\"\"\n",
        "        # 3D plot: parameter path on the error surface\n",
        "        ax = plt.axes(projection='3d')\n",
        "        ax.plot_wireframe(self.w, self.b, self.Z)\n",
        "        ax.scatter(self.W, self.B, self.LOSS, c='r', marker='x', s=200, alpha=1)\n",
        "        plt.title(\"Training Path on Error Surface (3D)\")\n",
        "        plt.xlabel('w')\n",
        "        plt.ylabel('b')\n",
        "        plt.show()\n",
        "        # 2D contour plot: parameter path\n",
        "        plt.figure()\n",
        "        plt.contour(self.w, self.b, self.Z)\n",
        "        plt.scatter(self.W, self.B, c='r', marker='x')\n",
        "        plt.xlabel('w')\n",
        "        plt.ylabel('b')\n",
        "        plt.title(\"Training Path on Error Surface (Contour)\")\n",
        "        plt.show()\n",
        "    \n",
        "    def plot_ps(self):\n",
        "        \"\"\"\n",
        "        At each training step, plot:\n",
        "        - The data space (showing the model's current fit versus the data)\n",
        "        - The parameter space (showing the trajectory of parameters on the error surface contour)\n",
        "        \"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        # Data space (left): how well do current w/b fit the data?\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.ylim((-10, 15))\n",
        "        plt.plot(self.x, self.y, 'ro', label=\"Training Points\")\n",
        "        plt.plot(self.x, self.W[-1] * self.x + self.B[-1], label=\"Estimated Line\")\n",
        "        plt.xlabel('x')\n",
        "        plt.ylabel('y')\n",
        "        plt.title(f'Data Space Iteration: {self.n}')\n",
        "        plt.legend()\n",
        "        # Parameter space (right): path on error surface\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.contour(self.w, self.b, self.Z)\n",
        "        plt.scatter(self.W, self.B, c='r', marker='x')\n",
        "        plt.title(f'Total Loss Surface Contour Iteration {self.n}')\n",
        "        plt.xlabel('w')\n",
        "        plt.ylabel('b')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Creating a Random Dataset\n",
        "\n",
        "To demonstrate linear regression, we'll generate a simple synthetic dataset.  \n",
        "We'll create input values $x$ from $-3$ to $3$, and generate outputs $y$ according to the true linear relationship $y = 1 \\cdot x - 1$, plus some random Gaussian noise to simulate real-world data.\n",
        "\n",
        "```python\n",
        "import torch  # PyTorch is used for tensors and automatic differentiation\n",
        "\n",
        "# Create input data: 1D tensor from -3 to 3 (step 0.1), reshaped as a column vector\n",
        "X = torch.arange(-3, 3, 0.1).view(-1, 1)\n",
        "\n",
        "# True function: y = x - 1\n",
        "f = 1 * X - 1\n",
        "\n",
        "# Add noise: Normal(0, 0.1) noise added to each y value\n",
        "Y = f + 0.1 * torch.randn(X.size())\n",
        "\n",
        "# Visualize the noisy data and the true line\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(X.numpy(), Y.numpy(), 'ro', label='Noisy Data')\n",
        "plt.plot(X.numpy(), f.numpy(), label='True Line')\n",
        "plt.title('Generated Data: Noisy Points and True Linear Relationship')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('y')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Defining the Model and Loss Function\n",
        "\n",
        "For linear regression, our prediction is:\n",
        "$$\n",
        "\\hat{y} = w \\cdot x + b\n",
        "$$\n",
        "where $w$ and $b$ are the parameters we want to learn.\n",
        "\n",
        "The **forward function** computes predictions for a given $x$.  \n",
        "The **criterion** function computes the mean squared error (MSE) loss between predictions and true values.\n",
        "\n",
        "```python\n",
        "def forward(x):\n",
        "    \"\"\"\n",
        "    Compute the predicted output y_hat for input x using current w and b.\n",
        "    \"\"\"\n",
        "    return w * x + b\n",
        "\n",
        "def criterion(yhat, y):\n",
        "    \"\"\"\n",
        "    Compute mean squared error between predictions (yhat) and true labels (y).\n",
        "    \"\"\"\n",
        "    return torch.mean((yhat - y) ** 2)\n",
        "```\n",
        "\n",
        "### Visualizing the Error Surface\n",
        "\n",
        "Before training, let's initialize the visualization class to view the error surface\n",
        "for our synthetic dataset.\n",
        "\n",
        "```python\n",
        "# This will plot the loss surface for a grid of (w, b) values\n",
        "get_surface = plot_error_surfaces(15, 15, X, Y, 30)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Training the Linear Regression Model\n",
        "\n",
        "Now we set up model parameters, the learning rate, and implement the training loop using gradient descent.\n",
        "\n",
        "- **Parameters**: $w$ (weight) and $b$ (bias), both initialized far from the optimum for demonstration.\n",
        "- **Learning Rate**: Controls the step size of each parameter update.\n",
        "- **Training Loop**: For a fixed number of epochs, we:\n",
        "  - Compute predictions and loss\n",
        "  - Record parameter values/loss for visualization\n",
        "  - Compute gradients via backpropagation\n",
        "  - Update parameters using gradient descent\n",
        "  - Zero gradients before next step\n",
        "\n",
        "```python\n",
        "# Initialize model parameters (w and b), with gradients enabled\n",
        "w = torch.tensor(-15.0, requires_grad=True)\n",
        "b = torch.tensor(-10.0, requires_grad=True)\n",
        "\n",
        "# Learning rate (step size for gradient descent)\n",
        "lr = 0.1\n",
        "\n",
        "# Track the loss at each iteration for plotting\n",
        "LOSS = []\n",
        "\n",
        "def train_model(iter):\n",
        "    \"\"\"\n",
        "    Train the model using gradient descent.\n",
        "    \n",
        "    Args:\n",
        "        iter (int): Number of epochs/iterations to train for.\n",
        "    \"\"\"\n",
        "    for epoch in range(iter):\n",
        "        Yhat = forward(X)                 # Predictions\n",
        "        loss = criterion(Yhat, Y)         # Loss (MSE)\n",
        "        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist()) # Record for visualization\n",
        "        if epoch % 3 == 0:\n",
        "            get_surface.plot_ps()         # Show fit and parameter path every 3 epochs\n",
        "        LOSS.append(loss)\n",
        "        loss.backward()                   # Compute gradients\n",
        "        # Gradient descent update (in-place, so PyTorch doesn't track this step)\n",
        "        w.data = w.data - lr * w.grad.data\n",
        "        b.data = b.data - lr * b.grad.data\n",
        "        # Zero gradients for next step\n",
        "        w.grad.data.zero_()\n",
        "        b.grad.data.zero_()\n",
        "\n",
        "# Train for 15 epochs\n",
        "train_model(15)\n",
        "\n",
        "# Visualize final parameter path on error surface\n",
        "get_surface.final_plot()\n",
        "\n",
        "# Plot the loss over epochs\n",
        "LOSS = [loss.detach().numpy() for loss in LOSS]\n",
        "plt.plot(LOSS)\n",
        "plt.tight_layout()\n",
        "plt.xlabel(\"Epoch/Iterations\")\n",
        "plt.ylabel(\"Cost (MSE Loss)\")\n",
        "plt.title(\"Loss Curve During Training\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**What do you see?**  \n",
        "- The training curve shows how the loss decreases as parameters are updated.\n",
        "- The error surface plots show how the parameter values ($w, b$) move toward the minimum of the loss surface.\n",
        "\n",
        "---\n",
        "\n",
        "## Experiment: Higher Learning Rate\n",
        "\n",
        "Let's see the effect of using a larger learning rate.\n",
        "\n",
        "- **Hypothesis:** A higher learning rate may converge faster, but if too high, might overshoot the minimum or cause instability.\n",
        "\n",
        "```python\n",
        "# Re-initialize parameters to same (bad) starting values\n",
        "w = torch.tensor(-15.0, requires_grad=True)\n",
        "b = torch.tensor(-10.0, requires_grad=True)\n",
        "lr = 0.2  # Double the learning rate\n",
        "LOSS2 = []\n",
        "\n",
        "def my_train_model(iter):\n",
        "    \"\"\"\n",
        "    Train the model using a different learning rate, storing loss in LOSS2.\n",
        "    \"\"\"\n",
        "    for epoch in range(iter):\n",
        "        Yhat = forward(X)\n",
        "        loss = criterion(Yhat, Y)\n",
        "        get_surface.set_para_loss(w.data.tolist(), b.data.tolist(), loss.tolist())\n",
        "        if epoch % 3 == 0:\n",
        "            get_surface.plot_ps()\n",
        "        LOSS2.append(loss)\n",
        "        loss.backward()\n",
        "        w.data = w.data - lr * w.grad.data\n",
        "        b.data = b.data - lr * b.grad.data\n",
        "        w.grad.data.zero_()\n",
        "        b.grad.data.zero_()\n",
        "\n",
        "# Train for 15 epochs with higher lr\n",
        "my_train_model(15)\n",
        "\n",
        "# Final visualization\n",
        "get_surface.final_plot()\n",
        "LOSS2 = [loss.detach().numpy() for loss in LOSS2]\n",
        "plt.plot(LOSS2)\n",
        "plt.tight_layout()\n",
        "plt.xlabel(\"Epoch/Iterations\")\n",
        "plt.ylabel(\"Cost (MSE Loss)\")\n",
        "plt.title(\"Loss Curve with Higher Learning Rate\")\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "**Questions to consider:**\n",
        "- Does the loss decrease more quickly?\n",
        "- Is the trajectory on the error surface smoother or more jagged?\n",
        "- Does increasing the learning rate even more become unstable?\n",
        "\n",
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "- We built a basic linear regression model in PyTorch from scratch.\n",
        "- Used gradient descent to optimize parameters.\n",
        "- Visualized both the data space (fit to data) and parameter space (error/loss surface and gradient descent path).\n",
        "- Saw how adjusting the learning rate affects model convergence.\n",
        "\n",
        "**Further exploration:**\n",
        "- Try different learning rates, initializations, or more iterations.\n",
        "- Try adding outliers or more noise to the data.\n",
        "- Modify the true function (change slope/intercept).\n",
        "- Extend to multiple features (multivariate regression).\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "pMIQ8iDka9-k"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IaDY4xRSa8UX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}